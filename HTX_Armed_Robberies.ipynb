{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''monthly robberies data set'''\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "#load data set\n",
    "data_series = pd.read_csv('./data/robberies.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "#split data set into model development data set and cross validation set, use last year of data as cv\n",
    "data_end = len(data_series) - 12\n",
    "data_set, cv_set = data_series[0:data_end], data_series[data_end::]\n",
    "print(f'Data set: {len(data_set)} months, Validation set: {len(cv_set)} months')\n",
    "\n",
    "#save training and cv sets to csv format\n",
    "data_set.to_csv('data_set.csv', header = False)\n",
    "cv_set.to_csv('cv_set.csv', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create training set and test set\n",
    "X = data_set.values\n",
    "X = X.astype('float32')\n",
    "\n",
    "#start with train set of 50%\n",
    "train_size = int(0.5*len(X))\n",
    "train_set, test_set = X[0:train_size], X[train_size::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use walk forward validation to create baseline prediction forecast using the persistence model\n",
    "#create foreacst history\n",
    "historic_obs = [x for x in train_set]\n",
    "#create baseline predictions\n",
    "predictions = []\n",
    "for i in range(len(test_set)):\n",
    "    #make prediction at t using observation at t-1\n",
    "    y_hat = historic_obs[-1]\n",
    "    predictions.append(y_hat)\n",
    "    #actual t observation from test set\n",
    "    observation = test_set[i]\n",
    "    #update historic observations with actual t observation\n",
    "    historic_obs.append(observation)\n",
    "    print(f'Predicted: {y_hat: .3f}, Expected: {observation: .3f}')\n",
    "#report the performance of the forecast using RMSE\n",
    "rmse = sqrt(mse(test_set, predictions))\n",
    "print(f'RMSE: {rmse: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics of data_set time series\n",
    "print(data_set.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data\n",
    "data_set.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line plot observations\n",
    "- There is a trend of increasing robberies\n",
    "- There is high variability among data points, which corresponds to the wide spread between the quartile ranges observed above\n",
    "- The variance between data points seem to increase over time, as observed from the widening ampitude of the fluctuations in the data\n",
    "- The data set is non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram and denisty plot\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "data_set.hist()\n",
    "plt.subplot(212)\n",
    "data_set.plot(kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and Density plot observations\n",
    "- The data distribution is not gaussian\n",
    "- The data seem to be quadratic or exponential, due to the left shift and the short right tail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group data by year for box plot analysis\n",
    "#1974 only has 10 months and not a full year, so we exclude it\n",
    "grps = data_set['1966':'1973'].groupby(pd.Grouper(freq='A'))\n",
    "yrs = pd.DataFrame({name.year: grp.values for name, grp in grps})\n",
    "yrs.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot observations\n",
    "- The earlier 2 years seem to have a much smaller variance, with the 1st and 3rd quantile being much closer to the median values.\n",
    "- The variance changes over time, but does not appear to do so consistently.\n",
    "- The median values do not exhibit a linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Implement an ARIMA model to forecast the number of robberies over time\n",
    "#first check for stationarity of the data set using the augmented Dickey-Fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_stationary(data):\n",
    "    '''function to check data stationarity using adfuller from statsmodel'''\n",
    "    result = adfuller(data)\n",
    "    print(f'ADF Statistic: {result[0]: .3f}')\n",
    "    print(f'p-value: {result[1]: .3f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "\n",
    "#check if stationary using Dickey-Fuller\n",
    "check_stationary(data_set.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dickey-Fuller Test observation:\n",
    "- The test statistic value is 0.797 , which is larger than the critical value of -2.893. This indicates that we cannot reject the null hypothesis, which claims that the data is non-stationary, with a significance level of less than 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employ first order differencing\n",
    "def differencing(data):\n",
    "    '''Function executes differencing of order 1 when called'''\n",
    "    differenced = [(data[i] - data[i-1]) for i in range(1, len(data))]\n",
    "    return differenced\n",
    "\n",
    "#executing differencing of order 1\n",
    "stationary_X = differencing(X)\n",
    "\n",
    "#check if the data is stationary now\n",
    "check_stationary(stationary_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The test statistic value is -3.981 , which is smaller than the critical value of -2.893. This indicates that we can reject the null hypothesis, which claims that the data is non-stationary, with a significance level of less than 5%.\n",
    "- Differencing by order of 1 seems to have made the data set stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ACF and PACF plots to check for autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plot_acf(X, lags=50, ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "plot_pacf(X, lags=50, ax=plt.gca())\n",
    "\n",
    "#fix any plot overlap with tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### ACF and PACF plot observations\n",
    "\n",
    "- The ACF plot shows that lags are significant for 10-12 months.\n",
    "- The PACF plot shows that lags are significant for likely just 2 months.\n",
    "- The PACF plot suggests that perhaps the autocorrelations at lag 3 and thereafter are due to the propagation of the autocorrelations at lags 1 and 2.\n",
    "- The plot suggest an ARMA(12,2) that can be used for modelling.\n",
    "- Since differencing order of the data is 1, an ARIMA(12,1,2) should be a good starting point for modelling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try an ARIMA model\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "arima1212_hist = [x for x in train_set]\n",
    "arima1212_pred = []\n",
    "for i in range(len(test_set)):\n",
    "    # predict\n",
    "    model = ARIMA(arima1212_hist, order=(0,1,2))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    arima1212_y_hat = model_fit.forecast()[0]\n",
    "    arima1212_pred.append(arima1212_y_hat)\n",
    "    # observation\n",
    "    obs = test_set[i]\n",
    "    arima1212_hist.append(obs)\n",
    "    print('>Predicted=%.3f, Expected=%.3f' % (arima1212_y_hat, obs))\n",
    "# report performance\n",
    "rmse = sqrt(mse(test_set, arima1212_pred))\n",
    "print(f'RMSE: {rmse: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Grid Search to find the optimal p,d,q hyperparameters\n",
    "def evaluate_arima_model(data, arima_order):\n",
    "    '''Function to evaluate data set X based on ARIMA order with some (p,d,q) and return RMSE'''\n",
    "    #prepare training data\n",
    "    data = data.astype('float32')\n",
    "    train_size = int(len(data) * 0.50)\n",
    "    train, test = data[0:train_size], data[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    rmse = sqrt(mse(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "def evaluate_models(data, p_val, d_val, q_val):\n",
    "    #convert data set to float to prevent numpy error\n",
    "    data = data.astype('float32')\n",
    "    best_rmse, best_config = float('inf'), None\n",
    "    #loop through all values of p,d,q to try every configuration order for ARIMA\n",
    "    for p in p_val:\n",
    "        for d in d_val:\n",
    "            for q in q_val:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(data, order)\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse, best_config = rmse, order\n",
    "                    print('ARIMA%s, RMSE = %.3f' % (order, rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s, RMSE = %.3f' % (best_config, best_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluate data set with all possible hyperparameter configurations\n",
    "p_val = range(0,13)\n",
    "d_val = range(0,2)\n",
    "q_val = range(0,13)\n",
    "\n",
    "#turn off verbose warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(X, p_val, d_val, q_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
